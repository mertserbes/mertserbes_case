{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPoIIdsWTFqzcVJphWJzEYJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mertserbes/mertserbes_case/blob/main/FinancialSentimentAnalysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Hasan Taskın 704241004\n",
        "#Mert Serbes 504241538"
      ],
      "metadata": {
        "id": "HILFoYHpesF1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kt23Uucredu7"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "def scrape_google_news(keywords=[\"BIST100\", \"Borsa İstanbul\", \"Hisse\"], limit=100):\n",
        "    news_data = []\n",
        "\n",
        "    headers = {\n",
        "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
        "    }\n",
        "\n",
        "    print(f\"Veri çekiliyor: {keywords}\")\n",
        "\n",
        "    for keyword in keywords:\n",
        "        # Google News RSS URL'si (Türkçe Haberler)\n",
        "        url = f\"https://news.google.com/rss/search?q={keyword}&hl=tr&gl=TR&ceid=TR:tr\"\n",
        "\n",
        "        try:\n",
        "            response = requests.get(url, headers=headers)\n",
        "            soup = BeautifulSoup(response.content, features=\"xml\") # XML parser kullanıyoruz\n",
        "\n",
        "            items = soup.find_all(\"item\")\n",
        "\n",
        "            print(f\"'{keyword}' için {len(items)} haber bulundu.\")\n",
        "\n",
        "            for item in items:\n",
        "                title = item.title.text\n",
        "                pub_date = item.pubDate.text\n",
        "                link = item.link.text\n",
        "                source = item.source.text if item.source else \"Bilinmiyor\"\n",
        "\n",
        "                # Sadece benzersiz başlıkları al\n",
        "                if not any(d['Title'] == title for d in news_data):\n",
        "                    news_data.append({\n",
        "                        \"Date\": pub_date,\n",
        "                        \"Title\": title,\n",
        "                        \"Source\": source,\n",
        "                        \"Keyword\": keyword,\n",
        "                        \"Label\": \"\" # Burayı sen dolduracaksın\n",
        "                    })\n",
        "\n",
        "                if len(news_data) >= limit:\n",
        "                    break\n",
        "\n",
        "            time.sleep(1) # Nezaketen bekleme\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Hata: {e}\")\n",
        "\n",
        "    df = pd.DataFrame(news_data)\n",
        "    return df\n",
        "\n",
        "# --- ÇALIŞTIR ---\n",
        "# 300-500 arası haber toplamaya çalış\n",
        "df_news = scrape_google_news(keywords=[\"BIST100\", \"Borsa\", \"Ekonomi\", \"Dolar TL\", \"Faiz Kararı\"], limit=500)\n",
        "\n",
        "print(f\"Toplam {len(df_news)} satır veri çekildi.\")\n",
        "print(df_news.head())\n",
        "\n",
        "# CSV Olarak Kaydet\n",
        "df_news.to_csv(\"bist100_sentiment_dataset.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install newspaper3k feedparser lxml_html_clean"
      ],
      "metadata": {
        "id": "y1bFwU6TjacT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import feedparser\n",
        "from newspaper import Article\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from urllib.parse import quote  # <-- YENİ EKLENEN KISIM: URL Kodlayıcı\n",
        "\n",
        "def scrape_full_content(keywords=[\"BIST100\", \"Borsa\", \"Hisse\"], limit_per_key=50):\n",
        "    data_list = []\n",
        "\n",
        "    # Kullanıcı gibi görünmek için User-Agent\n",
        "    config_headers = {\n",
        "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "    }\n",
        "\n",
        "    for keyword in keywords:\n",
        "        print(f\"\\n--- '{keyword}' için linkler toplanıyor ---\")\n",
        "\n",
        "        # --- DÜZELTME BURADA ---\n",
        "        # Kelimedeki boşlukları ve Türkçe karakterleri URL uyumlu hale getiriyoruz\n",
        "        # Örn: \"Borsa İstanbul\" -> \"Borsa%20%C4%B0stanbul\" olur.\n",
        "        encoded_keyword = quote(keyword)\n",
        "\n",
        "        rss_url = f\"https://news.google.com/rss/search?q={encoded_keyword}&hl=tr&gl=TR&ceid=TR:tr\"\n",
        "        # -----------------------\n",
        "\n",
        "        feed = feedparser.parse(rss_url)\n",
        "\n",
        "        print(f\"{len(feed.entries)} adet link bulundu. İçerikler indiriliyor...\")\n",
        "\n",
        "        count = 0\n",
        "        for entry in tqdm(feed.entries):\n",
        "            if count >= limit_per_key:\n",
        "                break\n",
        "\n",
        "            try:\n",
        "                url = entry.link\n",
        "\n",
        "                # Newspaper3k ile Haberin İçine Gir\n",
        "                article = Article(url)\n",
        "                article.download()\n",
        "                article.parse()\n",
        "\n",
        "                # Çok kısa metinleri atla\n",
        "                if len(article.text) < 50:\n",
        "                    continue\n",
        "\n",
        "                data_list.append({\n",
        "                    \"Date\": entry.published,\n",
        "                    \"Title\": article.title,\n",
        "                    \"Text\": article.text,\n",
        "                    \"Source\": entry.source.title if hasattr(entry, 'source') else \"Google News\",\n",
        "                    \"URL\": url\n",
        "                })\n",
        "\n",
        "                count += 1\n",
        "\n",
        "            except Exception as e:\n",
        "                # Bazı linkler hata verebilir, devam et\n",
        "                continue\n",
        "\n",
        "    return pd.DataFrame(data_list)\n",
        "\n",
        "# --- ÇALIŞTIR ---\n",
        "# Artık içinde boşluk olan kelimeler (\"Borsa İstanbul\" gibi) hata vermez.\n",
        "df_full = scrape_full_content(keywords=[\"BIST100\", \"Borsa İstanbul\", \"Ekonomi\"], limit_per_key=30)\n",
        "\n",
        "# Sonuçları Gör\n",
        "print(f\"\\nToplam {len(df_full)} satır veri çekildi.\")\n",
        "print(df_full.head())\n",
        "\n",
        "# Kaydet\n",
        "df_full.to_csv(\"bist100_full_text_dataset.csv\", index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IW3hELFzjYDZ",
        "outputId": "a3d91459-c08f-4b52-b7d1-34a3b3852179"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- 'BIST100' için linkler toplanıyor ---\n",
            "100 adet link bulundu. İçerikler indiriliyor...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:47<00:00,  2.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- 'Borsa İstanbul' için linkler toplanıyor ---\n",
            "100 adet link bulundu. İçerikler indiriliyor...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:44<00:00,  2.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- 'Ekonomi' için linkler toplanıyor ---\n",
            "102 adet link bulundu. İçerikler indiriliyor...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 102/102 [00:46<00:00,  2.20it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Toplam 0 satır veri çekildi.\n",
            "Empty DataFrame\n",
            "Columns: []\n",
            "Index: []\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}